

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.regularizers import l2
from tensorflow.keras.optimizers import Adam
from sklearn.preprocessing import PolynomialFeatures
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from tensorflow import keras
from tensorflow.keras import layers
from keras_tuner.tuners import RandomSearch
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.model_selection import GridSearchCV
from scipy.stats import pearsonr
from scipy.stats import bartlett
import seaborn as sns
import matplotlib.pyplot as plt
from kerastuner.tuners import BayesianOptimization
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from tensorflow import keras
from keras_tuner.tuners import BayesianOptimization
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras import layers
import json
import matplotlib.pyplot as plt

data = pd.read_csv('', sep=';')
data = pd.DataFrame(data)
data = data.dropna()


df = data.drop(columns=['Date', 'Rank', 'Country'])
Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)
IQR = Q3 - Q1
outliers = (df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))
print(outliers.sum())
sns.boxplot(df["Climate Index"])

top_outliers = data.nlargest(10, 'Safety Index')[['Country', 'Date', 'Safety Index']]
print(top_outliers)

top_outliers = data.nsmallest(10, 'Climate Index')[['Country', 'Date', 'Climate Index']]
print(top_outliers)

# Ð—Ð°Ð¼ÐµÐ½Ð° Ð½Ð° Ñ‡Ð¸ÑÐ»Ð¾Ð²Ñ‹Ðµ ÐºÐ¾Ð´Ñ‹
data['Country_Code'] = pd.factorize(data['Country'])[0]

# ÐŸÑ€ÐµÐ¾Ð±Ñ€Ð°Ð·Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð² datetime Ð¸ Ð·Ð°Ñ‚ÐµÐ¼ Ð² timestamp
data['date_num'] = pd.to_datetime(data['Date']).astype('int64') // 10**9

#data.head()
data[data['Country']=='Germany']

#df = data.drop(columns=['Date', 'Country', 'Is_Fire', 'Is_Crisis'])
df = data.drop(columns=['Date', 'Country'])
test = data.drop(columns=['Date', 'Country', 'Rank', 'Country_Code', 'date_num'])

plt.figure(figsize=(12, 10))
heatmap = sns.heatmap(test.corr(), annot=True, annot_kws={"size": 8},
                     cbar_kws={"shrink": 0.8}, fmt=".2f")
plt.xticks(rotation=45, ha='right')  # ÐŸÐ¾Ð²Ð¾Ñ€Ð¾Ñ‚ Ð¿Ð¾Ð´Ð¿Ð¸ÑÐµÐ¹ Ð¾ÑÐ¸ X
plt.yticks(rotation=0)
plt.tight_layout()  # ÐÐ²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ°Ñ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Ð¾Ñ‚ÑÑ‚ÑƒÐ¿Ð¾Ð²
plt.savefig("")

df_x = data.drop(columns=['Date', 'Country', 'Rank', 'Country_Code', 'date_num', 'Quality of Life Index'])

#Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð¼ÑƒÐ»ÑŒÑ‚Ð¸ÐºÐ¾Ð»Ð»Ð¸Ð½ÐµÐ°Ñ€Ð½Ð¾ÑÑ‚Ð¸
from statsmodels.stats.outliers_influence import variance_inflation_factor
vif_data = pd.DataFrame()
vif_data["ÐŸÑ€Ð¸Ð·Ð½Ð°Ðº"] = df_x.columns
vif_data["VIF"] = [variance_inflation_factor(df_x.values, i) for i in range(df_x.shape[1])]
print(vif_data)

df_x = df_x.drop(columns=['Health Care Index', 'Traffic Commute Time Index'])

from statsmodels.stats.outliers_influence import variance_inflation_factor

vif_data = pd.DataFrame()
vif_data["ÐŸÑ€Ð¸Ð·Ð½Ð°Ðº"] = df_x.columns
vif_data["VIF"] = [variance_inflation_factor(df_x.values, i) for i in range(df_x.shape[1])]
print(vif_data)

df_x['Economic_Stress'] = df['Cost of Living Index'] / df['Purchasing Power Index']
df_x = df_x.drop(columns=['Cost of Living Index', 'Purchasing Power Index'])

from statsmodels.stats.outliers_influence import variance_inflation_factor

vif_data = pd.DataFrame()
vif_data["ÐŸÑ€Ð¸Ð·Ð½Ð°Ðº"] = df_x.columns
vif_data["VIF"] = [variance_inflation_factor(df_x.values, i) for i in range(df_x.shape[1])]
print(vif_data)

from sklearn.decomposition import PCA
pca = PCA(n_components=1)
df_x['Safety_Pollution_PC'] = pca.fit_transform(df_x[['Safety Index', 'Pollution Index']])
df_x = df_x.drop(columns=['Safety Index', 'Pollution Index'])

print(pca.components_)

from statsmodels.stats.outliers_influence import variance_inflation_factor

vif_data = pd.DataFrame()
vif_data["ÐŸÑ€Ð¸Ð·Ð½Ð°Ðº"] = df_x.columns
vif_data["VIF"] = [variance_inflation_factor(df_x.values, i) for i in range(df_x.shape[1])]
print(vif_data)

plt.figure(figsize=(10, 8))
heatmap = sns.heatmap(df_x.corr(), annot=True, annot_kws={"size": 10},
                     cbar_kws={"shrink": 0.8}, fmt=".2f")
plt.xticks(rotation=45, ha='right')  # ÐŸÐ¾Ð²Ð¾Ñ€Ð¾Ñ‚ Ð¿Ð¾Ð´Ð¿Ð¸ÑÐµÐ¹ Ð¾ÑÐ¸ X
plt.yticks(rotation=0)
plt.tight_layout()  # ÐÐ²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ°Ñ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Ð¾Ñ‚ÑÑ‚ÑƒÐ¿Ð¾Ð²
plt.savefig("")

df_final = data.drop(columns=['Date', 'Country', 'Rank', 'Health Care Index', 'Traffic Commute Time Index'])
df_final['Economic_Stress'] = data['Cost of Living Index'] / data['Purchasing Power Index']
df_final = df_final.drop(columns=['Cost of Living Index', 'Purchasing Power Index'])
from sklearn.decomposition import PCA
pca = PCA(n_components=1)
df_final['Safety_Pollution_PC'] = pca.fit_transform(df_final[['Safety Index', 'Pollution Index']])
df_final = df_final.drop(columns=['Safety Index', 'Pollution Index'])
df_final.head()

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df_final)

#ÐšÑ€Ð¸Ñ‚ÐµÑ€Ð¸Ð¹ ÐšÐ°Ð¹Ð·ÐµÑ€Ð°-ÐœÐµÐ¹ÐµÑ€Ð°-ÐžÐ»ÐºÐ¸Ð½Ð° (KMO) Ð¸ Ð¢ÐµÑÑ‚ Ð‘Ð°Ñ€Ñ‚Ð»ÐµÑ‚Ñ‚Ð°
#KMO > 0.6 Ð¸ p-value < 0.05 â€” Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¿Ñ€Ð¸Ð³Ð¾Ð´Ð½Ñ‹ Ð´Ð»Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°.
from factor_analyzer import calculate_kmo, calculate_bartlett_sphericity
kmo_all, kmo_model = calculate_kmo(X_scaled)
bartlett, p_value = calculate_bartlett_sphericity(X_scaled)
print(f"KMO: {kmo_model}, Bartlett p-value: {p_value}")

df_final['Following QLI'] = None
for i in range(1589):
    current_country = df_final.loc[i, 'Country_Code']
    qli_list = [df_final.loc[i, 'Quality of Life Index']]
    temp = 1
    for j in range(i + 1, len(df_final)):
        if df_final.loc[j, 'Country_Code'] == current_country:
            qli_list.append(df_final.loc[j, 'Quality of Life Index'])
            temp += 1
        if temp == 3:
            break
    df_final.at[i, 'Following QLI'] = qli_list

df_final.head()

df_final = df_final.drop(columns=['Quality of Life Index'])

# Ð¤ÑƒÐ½ÐºÑ†Ð¸Ñ Ð´Ð»Ñ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ Ð¾Ñ‚Ñ€ÐµÐ·ÐºÐ¾Ð²
def create_segments(df, segment_length=5):
    segments = []
    labels = []

    # Ð£Ð±ÐµÐ´Ð¸Ð¼ÑÑ, Ñ‡Ñ‚Ð¾ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¾Ñ‚ÑÐ¾Ñ€Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ñ‹ Ð¿Ð¾ ÑÑ‚Ñ€Ð°Ð½Ðµ Ð¸ Ð´Ð°Ñ‚Ðµ
    df = df.sort_values(by=['Country_Code', 'date_num'])

    for country in df['Country_Code'].unique():
        country_data = df[df['Country_Code'] == country]
        for i in range(len(country_data) - segment_length + 1):
            # Ð’Ñ‹Ð±Ð¸Ñ€Ð°ÐµÐ¼ 5 ÑÑ‚Ñ€Ð¾Ðº Ð¿Ð¾Ð´Ñ€ÑÐ´
            segment = country_data.iloc[i:i+segment_length]
            # Ð£Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ð½ÐµÐ½ÑƒÐ¶Ð½Ñ‹Ðµ ÑÑ‚Ð¾Ð»Ð±Ñ†Ñ‹ (Following QLI, Country_Code, date_num)
            segment_data = segment.drop(columns=['Following QLI', 'Country_Code', 'date_num']).values
            # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼, Ñ‡Ñ‚Ð¾ Following QLI ÑÐ¾Ð´ÐµÑ€Ð¶Ð¸Ñ‚ Ñ€Ð¾Ð²Ð½Ð¾ 3 Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ
            following_qli = segment.iloc[-1]['Following QLI']
            if isinstance(following_qli, list) and len(following_qli) == 3:
                segments.append(segment_data)
                labels.append(following_qli)

    # ÐŸÑ€ÐµÐ¾Ð±Ñ€Ð°Ð·ÑƒÐµÐ¼ Ð² Ð¼Ð°ÑÑÐ¸Ð²Ñ‹ NumPy
    segments = np.array(segments)
    labels = np.array(labels)

    return segments, labels

# Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð¾Ñ‚Ñ€ÐµÐ·ÐºÐ¾Ð²
X, y = create_segments(df_final)

df_final[df['Country_Code']==0]

X[0]

y[0]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.preprocessing import StandardScaler, MinMaxScaler
# 1. Ð£Ð»ÑƒÑ‡ÑˆÐµÐ½Ð½Ð°Ñ Ð½Ð¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ (Ð¿Ð¾Ð¿Ñ€Ð¾Ð±ÑƒÐµÐ¼ MinMax Ð´Ð»Ñ Ð²Ñ‹Ñ…Ð¾Ð´Ð½Ñ‹Ñ…)
scaler_x = StandardScaler()
scaler_y = MinMaxScaler()

scaled_X_train = scaler_x.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)
scaled_X_test = scaler_x.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)

# ÐÐ¾Ñ€Ð¼Ð°Ð»Ð¸Ð·ÑƒÐµÐ¼ Ð²Ñ‹Ñ…Ð¾Ð´Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ ÐµÑÐ»Ð¸ Ð¾Ð½Ð¸ ÐµÑ‰Ðµ Ð½Ðµ Ð½Ð¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð¾Ð²Ð°Ð½Ñ‹
y_train_scaled = scaler_y.fit_transform(y_train)
y_test_scaled = scaler_y.transform(y_test)

print(scaled_X_train[0])

print(y)
print(scaler_y.fit_transform(y))

print(scaler_y.fit_transform(y))

# 1. ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð¸ Ð¿Ñ€ÐµÐ¾Ð±Ñ€Ð°Ð·Ð¾Ð²Ð°Ð½Ð¸Ðµ Ñ„Ð¾Ñ€Ð¼Ñ‹ Ð´Ð°Ð½Ð½Ñ‹Ñ…
print("Original X_train shape:", X_train.shape)

# 1. ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ…
print("Ð¤Ð¾Ñ€Ð¼Ð° Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð¾ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸:")
print(f"X_train: {X_train.shape}, y_train: {y_train.shape}")
print(f"X_test: {X_test.shape}, y_test: {y_test.shape}")

# 1. Ð£Ð¿Ñ€Ð¾Ñ‰ÐµÐ½Ð½Ð°Ñ Ð¸ ÑÑ‚Ð°Ð±Ð¸Ð»ÑŒÐ½Ð°Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒ
def build_model_0(hp):
    model = keras.Sequential()

    # ÐŸÑ€Ð¾ÑÑ‚Ð¾Ð¹ LSTM ÑÐ»Ð¾Ð¹
    model.add(layers.LSTM(
        units=hp.Int('units', min_value=16, max_value=64, step=16),
        input_shape=(scaled_X_train.shape[1], scaled_X_train.shape[2]),
        activation='tanh',  # Ð‘Ð¾Ð»ÐµÐµ ÑÑ‚Ð°Ð±Ð¸Ð»ÑŒÐ½Ð°Ñ Ð°ÐºÑ‚Ð¸Ð²Ð°Ñ†Ð¸Ñ
        return_sequences=False))

    # BatchNormalization Ð´Ð»Ñ ÑÑ‚Ð°Ð±Ð¸Ð»ÑŒÐ½Ð¾ÑÑ‚Ð¸
    model.add(layers.BatchNormalization())

    # ÐžÐ´Ð¸Ð½ Dense ÑÐ»Ð¾Ð¹
    model.add(layers.Dense(
        units=hp.Int('dense_units', min_value=8, max_value=32, step=8),
        activation='relu'))

    # Ð’Ñ‹Ñ…Ð¾Ð´Ð½Ð¾Ð¹ ÑÐ»Ð¾Ð¹
    model.add(layers.Dense(20, activation='linear'))

    # ÐšÐ¾Ð¼Ð¿Ð¸Ð»ÑÑ†Ð¸Ñ Ñ Ð¼ÐµÐ½ÑŒÑˆÐ¸Ð¼ learning rate
    model.compile(
        optimizer=keras.optimizers.Adam(
            hp.Float('lr', min_value=1e-4, max_value=1e-3, sampling='log')),
        loss='mse',
        metrics=['mae'])

    return model

# 2. Ð£Ð»ÑƒÑ‡ÑˆÐµÐ½Ð½Ð°Ñ Ð°Ñ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ð° Ð¼Ð¾Ð´ÐµÐ»Ð¸
def build_model(hp):
    model = keras.Sequential()

    # LSTM ÑÐ»Ð¾Ð¹ Ñ Ñ€ÐµÐ³ÑƒÐ»ÑÑ€Ð¸Ð·Ð°Ñ†Ð¸ÐµÐ¹
    model.add(layers.LSTM(
        units=hp.Int('units', 32, 128, step=16),
        input_shape=(scaled_X_train.shape[1], scaled_X_train.shape[2]),
        activation='tanh',
        recurrent_dropout=hp.Float('recurrent_dropout', 0.0, 0.3, step=0.1),
        return_sequences=False))

    model.add(layers.BatchNormalization())

    # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Dropout
    model.add(layers.Dropout(
        hp.Float('dropout_rate', 0.0, 0.5, step=0.1)))

    # Dense ÑÐ»Ð¾Ð¸
    for i in range(hp.Int('num_dense', 1, 2)):
        model.add(layers.Dense(
            units=hp.Int(f'dense_units_{i}', 16, 64, step=16),
            activation='relu',
            kernel_regularizer=keras.regularizers.l2(
                hp.Float('l2_reg', 1e-4, 1e-2, sampling='log'))))
        model.add(layers.BatchNormalization())

    # Ð’Ñ‹Ñ…Ð¾Ð´Ð½Ð¾Ð¹ ÑÐ»Ð¾Ð¹
    model.add(layers.Dense(20, activation='linear'))

    optimizer = keras.optimizers.Adam(
        learning_rate=hp.Float('lr', 1e-4, 1e-3, sampling='log'))

    model.compile(
        optimizer=optimizer,
        loss='mse',
        metrics=['mae'])

    return model

# 3. Ð£Ð»ÑƒÑ‡ÑˆÐµÐ½Ð½Ð°Ñ Ð°Ñ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ð° Ð¼Ð¾Ð´ÐµÐ»Ð¸
def build_model_3(hp):
    model = keras.Sequential()

    # LSTM ÑÐ»Ð¾Ð¹ Ñ Ñ€ÐµÐ³ÑƒÐ»ÑÑ€Ð¸Ð·Ð°Ñ†Ð¸ÐµÐ¹
    model.add(layers.LSTM(
        units=hp.Int('units', 32, 128, step=16),
        input_shape=(scaled_X_train.shape[1], scaled_X_train.shape[2]),
        activation='tanh',
        recurrent_dropout=hp.Float('recurrent_dropout', 0.0, 0.3, step=0.1),
        return_sequences=True))

    model.add(layers.BatchNormalization())

    # Dropout
    model.add(layers.Dropout(
        hp.Float('dropout_rate', 0.0, 0.5, step=0.1)))

    # LSTM 2 ÑÐ»Ð¾Ð¹ Ñ Ñ€ÐµÐ³ÑƒÐ»ÑÑ€Ð¸Ð·Ð°Ñ†Ð¸ÐµÐ¹
    model.add(layers.LSTM(
        units=hp.Int('units', 16, 64, step=16),
        input_shape=(scaled_X_train.shape[1], scaled_X_train.shape[2]),
        activation='tanh',
        recurrent_dropout=hp.Float('recurrent_dropout', 0.0, 0.3, step=0.1),
        return_sequences=False))

    model.add(layers.BatchNormalization())

    # Dropout
    model.add(layers.Dropout(
        hp.Float('dropout_rate', 0.0, 0.5, step=0.1)))

    # Dense ÑÐ»Ð¾Ð¸
    for i in range(hp.Int('num_dense', 1, 2)):
        model.add(layers.Dense(
            units=hp.Int(f'dense_units_{i}', 16, 64, step=16),
            activation='relu',
            kernel_regularizer=keras.regularizers.l2(
                hp.Float('l2_reg', 1e-4, 1e-2, sampling='log'))))
        model.add(layers.BatchNormalization())

    # Ð’Ñ‹Ñ…Ð¾Ð´Ð½Ð¾Ð¹ ÑÐ»Ð¾Ð¹
    model.add(layers.Dense(20, activation='linear'))

    optimizer = keras.optimizers.Adam(
        learning_rate=hp.Float('lr', 1e-4, 1e-3, sampling='log'))

    model.compile(
        optimizer=optimizer,
        loss='mse',
        metrics=['mae'])

    return model

def build_model_4(hp):
    model = keras.Sequential()

    # Ð”Ð²ÑƒÐ½Ð°Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð½Ð°Ñ LSTM
    model.add(layers.Bidirectional(
        layers.LSTM(
            hp.Int('units', 32, 128, step=32),
            recurrent_dropout=hp.Float('rec_drop', 0.1, 0.3)),
        input_shape=(scaled_X_train.shape[1], scaled_X_train.shape[2])))

    # Ð ÐµÐ³ÑƒÐ»ÑÑ€Ð¸Ð·Ð°Ñ†Ð¸Ñ
    model.add(layers.Dropout(hp.Float('dropout', 0.2, 0.5)))

    # Dense-ÑÐ»Ð¾Ð¸ Ñ L1/L2 Ñ€ÐµÐ³ÑƒÐ»ÑÑ€Ð¸Ð·Ð°Ñ†Ð¸ÐµÐ¹
    for i in range(hp.Int('num_dense', 1, 2)):
        model.add(layers.Dense(
            hp.Int(f'dense_{i}_units', 16, 64, step=16),
            kernel_regularizer=keras.regularizers.l1_l2(
                l1=hp.Float(f'l1_{i}', 1e-5, 1e-3),
                l2=hp.Float(f'l2_{i}', 1e-5, 1e-3))))

    # Ð’Ñ‹Ñ…Ð¾Ð´Ð½Ð¾Ð¹ ÑÐ»Ð¾Ð¹
    model.add(layers.Dense(20, activation='linear'))

    model.compile(
        optimizer=keras.optimizers.AdamW(
            hp.Float('lr', 1e-5, 1e-3),
            weight_decay=hp.Float('wd', 1e-6, 1e-4)),
        loss='mse',
        metrics=['mae'])
    return model

import numpy as np
import json
import matplotlib.pyplot as plt
import pandas as pd

from tensorflow import keras
from keras_tuner.tuners import BayesianOptimization
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras import layers

# --- 1. ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ ---
MODELS = {
    "Base_LSTM": build_model_0,
    "Regularized_LSTM": build_model,
    "Stacked_LSTM": build_model_2,
    "Bidirectional_LSTM": build_model_4,
}

# --- 2. Ð¤ÑƒÐ½ÐºÑ†Ð¸Ñ Ð´Ð»Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð¸ ÑÐ±Ð¾Ñ€Ð° Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð² ---
def run_hyperparameter_tuning(models, scaled_X_train, y_train_scaled, scaled_X_test, y_test_scaled):
    results = {}  # Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ
    all_histories = {}  # Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ

    for model_name, hypermodel in models.items():
        print(f"\nðŸ”§ Tuning {model_name}")

        tuner = BayesianOptimization(
            hypermodel,
            objective='val_loss',
            max_trials=15,
            num_initial_points=3,
            directory='tuning',
            project_name=model_name
        )

        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

        tuner.search(
            scaled_X_train,
            y_train_scaled,
            epochs=100,
            validation_split=0.2,
            callbacks=[early_stopping],
            verbose=1
        )

        best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]
        print(f"\nâœ… Best hyperparameters for {model_name}:")
        for param, value in best_hps.values.items():
            print(f"    {param}: {value}")

        best_model = tuner.hypermodel.build(best_hps)

        history = best_model.fit(
            scaled_X_train,
            y_train_scaled,
            epochs=100,
            validation_split=0.2,
            callbacks=[early_stopping],
            verbose=1
        )

        eval_result = best_model.evaluate(scaled_X_test, y_test_scaled)
        print(f"\nðŸ“ˆ Test loss for {model_name}: {eval_result}")

        y_pred = best_model.predict(scaled_X_test)
        results[model_name] = {
            'best': {
                'mse': mean_squared_error(y_test_scaled, y_pred),
                'mae': mean_absolute_error(y_test_scaled, y_pred),
                'params': best_hps.values,
                'epochs': len(history.history['val_loss']),
            }
        }

        all_histories[model_name] = history.history['val_loss']
        plot_model_history(model_name, history)

    plot_combined_val_loss(all_histories)

    # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð²ÑÐµ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð² JSON
    with open('', 'w') as f:
        json.dump(results, f, indent=2)

    return results

# --- 3. Ð¤ÑƒÐ½ÐºÑ†Ð¸Ñ Ð´Ð»Ñ Ð²Ð¸Ð·ÑƒÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ð¾Ð´Ð½Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸ ---
def plot_model_history(model_name, history):
    plt.figure(figsize=(10, 5))
    epochs = range(1, len(history.history['val_loss']) + 1)

    plt.plot(epochs, history.history['val_loss'], label='Validation Loss')
    plt.plot(epochs, history.history['loss'], label='Training Loss')
    plt.title(f'Ð˜Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¸ Ð¿Ð¾Ñ‚ÐµÑ€ÑŒ Ð¿Ð¾ ÑÐ¿Ð¾Ñ…Ð°Ð¼ Ð´Ð»Ñ {model_name}', pad=20)
    plt.xlabel('Ð­Ð¿Ð¾Ñ…Ð°', labelpad=10)
    plt.ylabel('Ð—Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¸ Ð¿Ð¾Ñ‚ÐµÑ€ÑŒ (MSE)', labelpad=10)
    plt.legend()
    plt.grid(True, linestyle='--', alpha=0.6)

    plt.tight_layout()
    plt.savefig(f'', dpi=120)
    plt.close()

# --- 4. Ð¤ÑƒÐ½ÐºÑ†Ð¸Ñ Ð´Ð»Ñ Ð¾Ð±Ñ‰ÐµÐ³Ð¾ ÑÑ€Ð°Ð²Ð½ÐµÐ½Ð¸Ñ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ ---
def plot_combined_val_loss(histories):
    plt.figure(figsize=(12, 8))

    max_epochs = max(len(h) for h in histories.values())

    for model_name, val_loss in histories.items():
        epochs = range(1, len(val_loss) + 1)
        plt.plot(epochs, val_loss, label=model_name, linewidth=2)

    plt.title('Ð¡Ñ€Ð°Ð²Ð½ÐµÐ½Ð¸Ðµ Ð´Ð¸Ð½Ð°Ð¼Ð¸ÐºÐ¸ Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ð¾Ð½Ð½Ð¾Ð¹ Ð¾ÑˆÐ¸Ð±ÐºÐ¸ (Loss) Ð´Ð»Ñ Ñ€Ð°Ð·Ð½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹', pad=20)
    plt.xlabel('Ð­Ð¿Ð¾Ñ…Ð°', labelpad=10)
    plt.ylabel('Ð—Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ð¾Ð½Ð½Ð¾Ð¹ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¸ Ð¿Ð¾Ñ‚ÐµÑ€ÑŒ (MSE)', labelpad=10)
    plt.legend()
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.xlim(1, max_epochs)

    plt.axhline(y=min(min(h) for h in histories.values()),
                color='gray', linestyle=':', alpha=0.5)

    plt.tight_layout()
    plt.savefig('',
                dpi=120, bbox_inches='tight')
    plt.close()

# --- 5. Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð² Excel ---
def save_results_to_excel(results):
    rows = []

    for model, data in results.items():
        rows.append({
            'Model': model,
            'MSE': data['best']['mse'],
            'MAE': data['best']['mae'],
            'Epochs': data['best']['epochs'],
            'Params': json.dumps(data['best']['params'])
        })

    df = pd.DataFrame(rows)
    df = df.sort_values(by='MSE')

    output_path = '/content/drive/MyDrive/8 ÑÐµÐ¼ÐµÑÑ‚Ñ€/Ð”Ð¸Ð¿Ð»Ð¾Ð¼/Ð’Ñ‹Ð²Ð¾Ð´/3/final_results.xlsx'
    df.to_excel(output_path, index=False)

    print(f"\nâœ… Excel Ñ„Ð°Ð¹Ð» ÑÐ¾Ñ…Ñ€Ð°Ð½Ñ‘Ð½ Ð¿Ð¾ Ð°Ð´Ñ€ÐµÑÑƒ: {output_path}")
    print("\nðŸ“Š TOP-3 Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð¿Ð¾ MSE:")
    print(df.head(3))

# --- 6. ÐŸÐ¾Ð»Ð½Ñ‹Ð¹ Ð·Ð°Ð¿ÑƒÑÐº Ð¿Ð°Ð¹Ð¿Ð»Ð°Ð¹Ð½Ð° ---
final_results = run_hyperparameter_tuning(MODELS, scaled_X_train, y_train_scaled, scaled_X_test, y_test_scaled)

# Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Excel
save_results_to_excel(final_results)

print("\nðŸš€ ÐŸÐ¾Ð»Ð½Ñ‹Ð¹ Ð¿Ñ€Ð¾Ñ†ÐµÑÑ Ð·Ð°Ð²ÐµÑ€ÑˆÑ‘Ð½!")

data = pd.read_csv('', sep=';')
data = pd.DataFrame(data)
data = data.dropna()
# Ð—Ð°Ð¼ÐµÐ½Ð° Ð½Ð° Ñ‡Ð¸ÑÐ»Ð¾Ð²Ñ‹Ðµ ÐºÐ¾Ð´Ñ‹
data['Country_Code'] = pd.factorize(data['Country'])[0]
# ÐŸÑ€ÐµÐ¾Ð±Ñ€Ð°Ð·Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð² datetime Ð¸ Ð·Ð°Ñ‚ÐµÐ¼ Ð² timestamp
data['date_num'] = pd.to_datetime(data['Date']).astype('int64') // 10**9
df_final = data.drop(columns=['Date', 'Country', 'Rank', 'Health Care Index', 'Traffic Commute Time Index'])
df_final['Economic_Stress'] = data['Cost of Living Index'] / data['Purchasing Power Index']
df_final = df_final.drop(columns=['Cost of Living Index', 'Purchasing Power Index'])
pca = PCA(n_components=1)
df_final['Safety_Pollution_PC'] = pca.fit_transform(df_final[['Safety Index', 'Pollution Index']])
df_final = df_final.drop(columns=['Safety Index', 'Pollution Index'])
df_final.head()
df_final['Following QLI'] = None
for i in range(1422):
    current_country = df_final.loc[i, 'Country_Code']
    qli_list = [df_final.loc[i, 'Quality of Life Index']]
    temp = 1
    for j in range(i + 1, len(df_final)):
        if df_final.loc[j, 'Country_Code'] == current_country:
            qli_list.append(df_final.loc[j, 'Quality of Life Index'])
            temp += 1
        if temp == 3:
            break
    df_final.at[i, 'Following QLI'] = qli_list
df_final.head()
df_final = df_final.drop(columns=['Quality of Life Index'])
# Ð¤ÑƒÐ½ÐºÑ†Ð¸Ñ Ð´Ð»Ñ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ Ð¾Ñ‚Ñ€ÐµÐ·ÐºÐ¾Ð²
def create_segments(df, segment_length=5):
    segments = []
    labels = []

    # Ð£Ð±ÐµÐ´Ð¸Ð¼ÑÑ, Ñ‡Ñ‚Ð¾ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¾Ñ‚ÑÐ¾Ñ€Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ñ‹ Ð¿Ð¾ ÑÑ‚Ñ€Ð°Ð½Ðµ Ð¸ Ð´Ð°Ñ‚Ðµ
    df = df.sort_values(by=['Country_Code', 'date_num'])

    for country in df['Country_Code'].unique():
        country_data = df[df['Country_Code'] == country]
        for i in range(len(country_data) - segment_length + 1):
            # Ð’Ñ‹Ð±Ð¸Ñ€Ð°ÐµÐ¼ 5 ÑÑ‚Ñ€Ð¾Ðº Ð¿Ð¾Ð´Ñ€ÑÐ´
            segment = country_data.iloc[i:i+segment_length]
            # Ð£Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ð½ÐµÐ½ÑƒÐ¶Ð½Ñ‹Ðµ ÑÑ‚Ð¾Ð»Ð±Ñ†Ñ‹ (Following QLI, Country_Code, date_num)
            segment_data = segment.drop(columns=['Following QLI', 'Country_Code', 'date_num']).values
            # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼, Ñ‡Ñ‚Ð¾ Following QLI ÑÐ¾Ð´ÐµÑ€Ð¶Ð¸Ñ‚ Ñ€Ð¾Ð²Ð½Ð¾ 5 Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ
            following_qli = segment.iloc[-1]['Following QLI']
            if isinstance(following_qli, list) and len(following_qli) == 3:
                segments.append(segment_data)
                labels.append(following_qli)

    # ÐŸÑ€ÐµÐ¾Ð±Ñ€Ð°Ð·ÑƒÐµÐ¼ Ð² Ð¼Ð°ÑÑÐ¸Ð²Ñ‹ NumPy
    segments = np.array(segments)
    labels = np.array(labels)

    return segments, labels

# Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð¾Ñ‚Ñ€ÐµÐ·ÐºÐ¾Ð²
X, y = create_segments(df_final)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 1. Ð£Ð»ÑƒÑ‡ÑˆÐµÐ½Ð½Ð°Ñ Ð½Ð¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ (Ð¿Ð¾Ð¿Ñ€Ð¾Ð±ÑƒÐµÐ¼ MinMax Ð´Ð»Ñ Ð²Ñ‹Ñ…Ð¾Ð´Ð½Ñ‹Ñ…)
scaler_x = StandardScaler()
scaler_y = MinMaxScaler()

scaled_X_train = scaler_x.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)
scaled_X_test = scaler_x.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)

# ÐÐ¾Ñ€Ð¼Ð°Ð»Ð¸Ð·ÑƒÐµÐ¼ Ð²Ñ‹Ñ…Ð¾Ð´Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ ÐµÑÐ»Ð¸ Ð¾Ð½Ð¸ ÐµÑ‰Ðµ Ð½Ðµ Ð½Ð¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð¾Ð²Ð°Ð½Ñ‹
y_train_scaled = scaler_y.fit_transform(y_train)
y_test_scaled = scaler_y.transform(y_test)

import numpy as np
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.model_selection import GridSearchCV

# Ð²Ñ…Ð¾Ð´Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ ÑƒÐ¶Ðµ Ð¿Ð¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²Ð»ÐµÐ½Ñ‹:
def create_lagged_features(data, n_lags):
    n_samples, n_timesteps, n_features = data.shape
    X_lagged = np.zeros((n_samples, n_lags * n_features))

    for i in range(n_samples):
        for lag in range(n_lags):
            start_idx = lag * n_features
            end_idx = (lag + 1) * n_features
            X_lagged[i, start_idx:end_idx] = data[i, - (lag + 1), :]  # Ð‘ÐµÑ€Ñ‘Ð¼ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ðµ `n_lags` ÑˆÐ°Ð³Ð¾Ð²

    return X_lagged

n_lags = 3  # Ð¡ÐºÐ¾Ð»ÑŒÐºÐ¾ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ñ… ÑˆÐ°Ð³Ð¾Ð² ÑƒÑ‡Ð¸Ñ‚Ñ‹Ð²Ð°Ñ‚ÑŒ
X_train_rf = create_lagged_features(scaled_X_train, n_lags)
X_test_rf = create_lagged_features(scaled_X_test, n_lags)


# ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð¸ Ð¸Ñ… ÑÐµÑ‚ÐºÐ¸ Ð³Ð¸Ð¿ÐµÑ€Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¾Ð²
model_params = {
    'Linear Regression': {
        'model': LinearRegression(),
        'params': {}  # ÐÐµÑ‚ Ð³Ð¸Ð¿ÐµÑ€Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¾Ð² Ð´Ð»Ñ Ð¿ÐµÑ€ÐµÐ±Ð¾Ñ€Ð°
    },
    'Ridge Regression': {
        'model': Ridge(),
        'params': {'alpha': [0.01, 0.1, 1.0, 10.0, 100.0]}
    },
    'Lasso Regression': {
        'model': Lasso(),
        'params': {'alpha': [0.01, 0.1, 1.0, 10.0, 100.0]}
    },
    'Polynomial Regression': {
        'model': PolynomialFeatures(),
        'params': {
            'degree': [2, 3, 4]  # Ð¡Ñ‚ÐµÐ¿ÐµÐ½ÑŒ Ð¿Ð¾Ð»Ð¸Ð½Ð¾Ð¼Ð° Ð´Ð»Ñ Ñ€ÐµÐ³Ñ€ÐµÑÑÐ¸Ð¸
        }
    }
}

# ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ñ Ð¿Ð¾Ð´Ð±Ð¾Ñ€Ð¾Ð¼ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¾Ð²
for name, mp in model_params.items():
    print(f'ÐŸÐ¾Ð´Ð±Ð¾Ñ€ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¾Ð² Ð´Ð»Ñ {name}...')

    if name == 'Polynomial Regression':
        # Ð”Ð»Ñ Ð¿Ð¾Ð»Ð¸Ð½Ð¾Ð¼Ð¸Ð°Ð»ÑŒÐ½Ð¾Ð¹ Ñ€ÐµÐ³Ñ€ÐµÑÑÐ¸Ð¸, Ð¿Ñ€ÐµÐ¾Ð±Ñ€Ð°Ð·ÑƒÐµÐ¼ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¸ Ð¿ÐµÑ€ÐµÐ´ Ð»Ð¸Ð½ÐµÐ¹Ð½Ð¾Ð¹ Ñ€ÐµÐ³Ñ€ÐµÑÑÐ¸ÐµÐ¹
        poly = PolynomialFeatures(degree=mp['params']['degree'][0])  # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð¿ÐµÑ€Ð²Ñ‹Ð¹ ÑƒÑ€Ð¾Ð²ÐµÐ½ÑŒ Ð¸Ð· Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¾Ð²
        X_poly_train = poly.fit_transform(X_train_rf)
        X_poly_test = poly.transform(X_test_rf)

        model = LinearRegression()
        model.fit(X_poly_train, y_train_scaled)

        y_pred = model.predict(X_poly_test)
    else:
        grid = GridSearchCV(mp['model'], mp['params'], cv=3, scoring='neg_mean_squared_error', n_jobs=-1)
        grid.fit(X_train_rf, y_train_scaled)

        best_model = grid.best_estimator_
        y_pred = best_model.predict(X_test_rf)

    mse = mean_squared_error(y_test_scaled, y_pred)
    mae = mean_absolute_error(y_test_scaled, y_pred)

    print(f'{name}:')
    print(f'  Ð›ÑƒÑ‡ÑˆÐ¸Ðµ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹: {grid.best_params_}')
    print(f'  MSE: {mse:.8f}')
    print(f'  MAE: {mae:.8f}\n')



import matplotlib.pyplot as plt
import random

indices = random.sample(range(len(y_test_scaled_10)), num_examples)

# ÐŸÐ¾ÑÑ‚Ñ€Ð¾Ð¸Ð¼ Ð³Ñ€Ð°Ñ„Ð¸ÐºÐ¸
for idx in indices:
    plt.figure(figsize=(12, 6))
    plt.plot(y_test_scaled_10[idx], label='Ð ÐµÐ°Ð»ÑŒÐ½Ñ‹Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ', linewidth=2, color='black')

    for name, y_pred in results.items():
        plt.plot(y_pred[idx], label=name)

    plt.title(f'ÐŸÑ€Ð¾Ð³Ð½Ð¾Ð· Ð½Ð° 5 Ð»ÐµÑ‚ (Ð¿Ñ€Ð¸Ð¼ÐµÑ€ {idx})')
    plt.xlabel('Ð“Ð¾Ñ€Ð¸Ð·Ð¾Ð½Ñ‚ Ð¿Ñ€Ð¾Ð³Ð½Ð¾Ð·Ð° (ÑˆÐ°Ð³)')
    plt.ylabel('Ð—Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ (Ð½Ð¾Ñ€Ð¼.)')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

def create_lagged_features(data, n_lags):
    n_samples, n_timesteps, n_features = data.shape
    X_lagged = np.zeros((n_samples, n_lags * n_features))

    for i in range(n_samples):
        for lag in range(n_lags):
            start_idx = lag * n_features
            end_idx = (lag + 1) * n_features
            X_lagged[i, start_idx:end_idx] = data[i, - (lag + 1), :]  # Ð‘ÐµÑ€Ñ‘Ð¼ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ðµ `n_lags` ÑˆÐ°Ð³Ð¾Ð²

    return X_lagged

# ÐŸÑ€Ð¸Ð¼ÐµÑ€:
n_lags = 3  # Ð¡ÐºÐ¾Ð»ÑŒÐºÐ¾ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ñ… ÑˆÐ°Ð³Ð¾Ð² ÑƒÑ‡Ð¸Ñ‚Ñ‹Ð²Ð°Ñ‚ÑŒ
X_train_rf_10 = create_lagged_features(scaled_X_train_10, n_lags)
X_test_rf_10 = create_lagged_features(scaled_X_test_10, n_lags)

print("X_train_rf shape:", X_train_rf.shape)  # (n_samples, n_lags * n_features)

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error, mean_absolute_error

# ÐŸÐ°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ Ð´Ð»Ñ GridSearch
param_grid = {
      'n_estimators': [30, 50, 100],
    'max_depth': [None, 5, 10],
    'min_samples_split': [5, 10, 15],
}

# ÐŸÐ¾Ð¸ÑÐº Ð»ÑƒÑ‡ÑˆÐµÐ¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸
rf_model = RandomForestRegressor(random_state=42)
grid_search = GridSearchCV(rf_model, param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(X_train_rf_10, y_train_scaled_10)

# Ð›ÑƒÑ‡ÑˆÐ°Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒ
best_rf_10 = grid_search.best_estimator_
y_pred_rf_10 = best_rf_10.predict(X_test_rf_10)

# ÐžÑ†ÐµÐ½ÐºÐ° ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð°
mse_rf_10 = mean_squared_error(y_test_scaled_10, y_pred_rf_10)
mae_rf_10 = mean_absolute_error(y_test_scaled_10, y_pred_rf_10)

print(f"Random Forest: MSE = {mse_rf_10:.4f}, MAE = {mae_rf_10:.4f}")

# ÐŸÐ¾ÑÑ‚Ñ€Ð¾Ð¸Ð¼ Ð³Ñ€Ð°Ñ„Ð¸ÐºÐ¸
for idx in indices:
    plt.figure(figsize=(12, 6))
    plt.plot(y_test_scaled_20[idx], label='Ð ÐµÐ°Ð»ÑŒÐ½Ñ‹Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ', linewidth=2, color='black')

    for name, y_pred in results.items():
        plt.plot(y_pred[idx], label=name)

    plt.title(f'ÐŸÑ€Ð¾Ð³Ð½Ð¾Ð· Ð½Ð° 10 Ð»ÐµÑ‚ (Ð¿Ñ€Ð¸Ð¼ÐµÑ€ {idx})')
    plt.xlabel('Ð“Ð¾Ñ€Ð¸Ð·Ð¾Ð½Ñ‚ Ð¿Ñ€Ð¾Ð³Ð½Ð¾Ð·Ð° (ÑˆÐ°Ð³)')
    plt.ylabel('Ð—Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ (Ð½Ð¾Ñ€Ð¼.)')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

def create_lagged_features(data, n_lags):
    n_samples, n_timesteps, n_features = data.shape
    X_lagged = np.zeros((n_samples, n_lags * n_features))

    for i in range(n_samples):
        for lag in range(n_lags):
            start_idx = lag * n_features
            end_idx = (lag + 1) * n_features
            X_lagged[i, start_idx:end_idx] = data[i, - (lag + 1), :]  # Ð‘ÐµÑ€Ñ‘Ð¼ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ðµ `n_lags` ÑˆÐ°Ð³Ð¾Ð²

    return X_lagged

# ÐŸÑ€Ð¸Ð¼ÐµÑ€:
n_lags = 3  # Ð¡ÐºÐ¾Ð»ÑŒÐºÐ¾ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ñ… ÑˆÐ°Ð³Ð¾Ð² ÑƒÑ‡Ð¸Ñ‚Ñ‹Ð²Ð°Ñ‚ÑŒ
X_train_rf_20 = create_lagged_features(scaled_X_train_20, n_lags)
X_test_rf_20 = create_lagged_features(scaled_X_test_20, n_lags)

# ÐŸÐ°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ Ð´Ð»Ñ GridSearch
param_grid = {
      'n_estimators': [30, 50, 100],
    'max_depth': [None, 5, 10],
    'min_samples_split': [5, 10, 15],
}

# ÐŸÐ¾Ð¸ÑÐº Ð»ÑƒÑ‡ÑˆÐµÐ¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸
rf_model = RandomForestRegressor(random_state=42)
grid_search = GridSearchCV(rf_model, param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(X_train_rf_20, y_train_scaled_20)

# Ð›ÑƒÑ‡ÑˆÐ°Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒ
best_rf_20 = grid_search.best_estimator_
y_pred_rf_20 = best_rf_20.predict(X_test_rf_20)

# ÐžÑ†ÐµÐ½ÐºÐ° ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð°
mse_rf_20 = mean_squared_error(y_test_scaled_20, y_pred_rf_20)
mae_rf_20 = mean_absolute_error(y_test_scaled_20, y_pred_rf_20)

print(f"Random Forest: MSE = {mse_rf_20:.10f}, MAE = {mae_rf_20:.10f}")

print(f"Random Forest: MSE = {mse_rf:.10f}, MAE = {mae_rf:.10f}")
print(f"Random Forest 5: MSE = {mse_rf_5:.10f}, MAE = {mae_rf_5:.10f}")
print(f"Random Forest 10: MSE = {mse_rf_10:.10f}, MAE = {mae_rf_10:.10f}")
print(f"Random Forest 20: MSE = {mse_rf_20:.10f}, MAE = {mae_rf_20:.10f}")

from tabulate import tabulate

# Ð˜ÑÑ…Ð¾Ð´Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹
models = [
    {
        "Model": "Base_LSTM",
        "units": 16,
        "dense_units": 32,
        "lr": 0.000463
    },
    {
        "Model": "Regularized_LSTM",
        "units": 96,
        "recurrent_dropout": 0.0,
        "dropout": 0.4,
        "dense_layers": 1,
        "dense_units": 16,
        "l2": 0.005287,
        "lr": 0.001
    },
    {
        "Model": "Stacked_LSTM",
        "units": 64,
        "recurrent_dropout": 0.1,
        "dropout": 0.2,
        "dense_layers": 1,
        "dense_units": 48,
        "l2": 0.0001,
        "lr": 0.000794
    },
    {
        "Model": "Bidirectional_LSTM",
        "units": 32,
        "recurrent_dropout": 0.1,
        "dropout": 0.5,
        "dense_layers": 1,
        "dense_units": 16,
        "l1": 0.00001,
        "l2": 0.000797,
        "lr": 0.001,
    }
]

# Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ ÐºÑ€Ð°ÑÐ¸Ð²ÑƒÑŽ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñƒ
table_data = []
headers = [
    "ÐœÐ¾Ð´ÐµÐ»ÑŒ",
    "ÐšÐ¾Ð»-Ð²Ð¾ Ð½ÐµÐ¹Ñ€Ð¾Ð½Ð¾Ð²\nÐ² LSTM-ÑÐ»Ð¾Ðµ",
    "Recurrent\ndropout",
    "Dropout Ð½Ð°\nÐ²Ñ‹Ñ…Ð¾Ð´Ð°Ñ… LSTM",
    "ÐšÐ¾Ð»-Ð²Ð¾ Ð½ÐµÐ¹Ñ€Ð¾Ð½Ð¾Ð²\nÐ² Dense ÑÐ»Ð¾Ðµ",
    "L1-Ñ€ÐµÐ³ÑƒÐ»ÑÑ€Ð¸Ð·Ð°Ñ†Ð¸Ñ",
    "L2-Ñ€ÐµÐ³ÑƒÐ»ÑÑ€Ð¸Ð·Ð°Ñ†Ð¸Ñ",
    "Ð¡ÐºÐ¾Ñ€Ð¾ÑÑ‚ÑŒ\nÐ¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ (lr)"
]

for model in models:
    row = [
        model["Model"],
        model.get("units", ""),
        model.get("recurrent_dropout", "-"),
        model.get("dropout", "-"),
        model.get("dense_units", "-"),
        model.get("l1", "-"),
        model.get("l2", "-"),
        model.get("lr", "-")
    ]
    table_data.append(row)

# Ð’Ñ‹Ð²Ð¾Ð´Ð¸Ð¼ ÑÑ‚Ð¸Ð»ÑŒÐ½ÑƒÑŽ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñƒ Ñ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ°Ð¼Ð¸
print(tabulate(
    table_data,
    headers=headers,
    tablefmt="grid",
    numalign="center",
    stralign="center",
    floatfmt=".6f"
))

import matplotlib.pyplot as plt
import numpy as np

# Ð“Ð¾Ñ€Ð¸Ð·Ð¾Ð½Ñ‚ Ð¿Ñ€Ð¾Ð³Ð½Ð¾Ð·Ð° Ð² Ð¿ÐµÑ€Ð¸Ð¾Ð´Ð°Ñ…
horizons = [1.5, 2.5, 5, 10]

# ÐžÑˆÐ¸Ð±ÐºÐ¸ MSE Ð´Ð»Ñ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹
mse_data = {
    'Bidirectional_LSTM': [0.002640654, 0.003889505, 0.00660325, 0.011556428],
    'Base_LSTM':          [0.002964542, 0.009032848, 0.008671352, 0.031456055],
    'Regularized_LSTM':   [0.003725677, 0.005479769, 0.007314557, 0.020705814],
    'Stacked_LSTM':       [0.004086467, 0.004830991, 0.008041487, 0.039676043],
    'Lasso Regression':   [0.00867619,  0.01093151,  0.00733351,  0.01234252],
    #'Polynomial Regression': [2.22574551,  3.17753427,  0.01066679,  0.25810581],
    'Linear Regression':  [0.02175156,  0.02194493, 0.00744146,  0.01335177],
    'Ridge Regression':   [0.02174603,  0.02177307, 0.00763521,  0.01201772]
}

# Ð¦Ð²ÐµÑ‚Ð° Ð´Ð»Ñ ÐºÑ€Ð°ÑÐ¾Ñ‚Ñ‹
colors = {
    'Bidirectional_LSTM': '#1f77b4',
    'Base_LSTM':          '#aec7e8',
    'Regularized_LSTM':   '#ff7f0e',
    'Stacked_LSTM':       '#ffbb78',
    'Lasso Regression':      '#2ca02c',
    #'Polynomial Regression':            '#98df8a',
    'Linear Regression':  '#d62728',
    'Ridge Regression':   '#ff9896'
}

# Ð¡Ñ‚Ñ€Ð¾Ð¸Ð¼ Ð³Ñ€Ð°Ñ„Ð¸Ðº
plt.figure(figsize=(12, 7))

for model_name, errors in mse_data.items():
    plt.plot(horizons, errors, marker='o', label=model_name, color=colors.get(model_name, None))

plt.title('Ð¡Ñ€Ð°Ð²Ð½ÐµÐ½Ð¸Ðµ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð¿Ð¾ MSE Ð² Ð·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑ‚Ð¸ Ð¾Ñ‚ Ð³Ð¾Ñ€Ð¸Ð·Ð¾Ð½Ñ‚Ð° Ð¿Ñ€Ð¾Ð³Ð½Ð¾Ð·Ð°', fontsize=16)
plt.xlabel('Ð“Ð¾Ñ€Ð¸Ð·Ð¾Ð½Ñ‚ Ð¿Ñ€Ð¾Ð³Ð½Ð¾Ð·Ð° (Ð¿ÐµÑ€Ð¸Ð¾Ð´Ñ‹)', fontsize=14)
plt.ylabel('ÐžÑˆÐ¸Ð±ÐºÐ° (MSE)', fontsize=14)
plt.grid(True)
plt.legend()
plt.xticks(horizons)
plt.savefig('/content/drive/MyDrive/8 ÑÐµÐ¼ÐµÑÑ‚Ñ€/Ð”Ð¸Ð¿Ð»Ð¾Ð¼/Ð’Ñ‹Ð²Ð¾Ð´/mse_comparison.png',
                dpi=120, bbox_inches='tight')
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Ð“Ð¾Ñ€Ð¸Ð·Ð¾Ð½Ñ‚ Ð¿Ñ€Ð¾Ð³Ð½Ð¾Ð·Ð° Ð² Ð¿ÐµÑ€Ð¸Ð¾Ð´Ð°Ñ…
horizons = [3, 5, 10, 20]

# ÐžÑˆÐ¸Ð±ÐºÐ¸ MSE Ð´Ð»Ñ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹
mse_data = {
    'Bidirectional_LSTM': [0.002640654, 0.003889505, 0.00660325, 0.011556428],
    'Base_LSTM':          [0.002964542, 0.009032848, 0.008671352, 0.031456055],
    'Regularized_LSTM':   [0.003725677, 0.005479769, 0.007314557, 0.020705814],
    'Stacked_LSTM':       [0.004086467, 0.004830991, 0.008041487, 0.039676043],
    'Lasso Regression':   [0.00867619,  0.01093151,  0.00733351,  0.01234252],
    #'Polynomial Regression': [2.22574551,  3.17753427,  0.01066679,  0.25810581],
    'Linear Regression':  [0.02175156,  0.02194493, 0.00744146,  0.01335177],
    'Ridge Regression':   [0.02174603,  0.02177307, 0.00763521,  0.01201772]
}

# Ð¦Ð²ÐµÑ‚Ð° Ð´Ð»Ñ ÐºÑ€Ð°ÑÐ¾Ñ‚Ñ‹
colors = {
    'Bidirectional_LSTM': '#1f77b4',
    'Base_LSTM':          '#1f77b4',
    'Regularized_LSTM':   '#1f77b4',
    'Stacked_LSTM':       '#1f77b4',
    'Lasso Regression':      '#2ca02c',
    #'Polynomial Regression':            '#98df8a',
    'Linear Regression':  '#2ca02c',
    'Ridge Regression':   '#2ca02c'
}

# Ð¡Ñ‚Ñ€Ð¾Ð¸Ð¼ Ð³Ñ€Ð°Ñ„Ð¸Ðº
plt.figure(figsize=(12, 7))

for model_name, errors in mse_data.items():
    plt.plot(horizons, errors, marker='o', label=model_name, color=colors.get(model_name, None))

plt.title('Ð¡Ñ€Ð°Ð²Ð½ÐµÐ½Ð¸Ðµ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð¿Ð¾ MSE Ð² Ð·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑ‚Ð¸ Ð¾Ñ‚ Ð³Ð¾Ñ€Ð¸Ð·Ð¾Ð½Ñ‚Ð° Ð¿Ñ€Ð¾Ð³Ð½Ð¾Ð·Ð°', fontsize=16)
plt.xlabel('Ð“Ð¾Ñ€Ð¸Ð·Ð¾Ð½Ñ‚ Ð¿Ñ€Ð¾Ð³Ð½Ð¾Ð·Ð° (Ð¿ÐµÑ€Ð¸Ð¾Ð´Ñ‹)', fontsize=14)
plt.ylabel('ÐžÑˆÐ¸Ð±ÐºÐ° (MSE)', fontsize=14)
plt.grid(True)
plt.legend()
plt.xticks(horizons)
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Ð“Ð¾Ñ€Ð¸Ð·Ð¾Ð½Ñ‚ Ð¿Ñ€Ð¾Ð³Ð½Ð¾Ð·Ð° Ð² Ð¿ÐµÑ€Ð¸Ð¾Ð´Ð°Ñ…
horizons = [1.5, 2.5, 5, 10]

# ÐžÑˆÐ¸Ð±ÐºÐ¸ MSE Ð´Ð»Ñ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹
mse_data = {
    'Ð”Ð²ÑƒÐ½Ð°Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð½Ð°Ñ LSTM': [6.9, 7.1, 8.1, 10.9],
    'Ð£Ð¿Ñ€Ð¾Ñ‰ÐµÐ½Ð½Ð°Ñ Ð±Ð°Ð·Ð¾Ð²Ð°Ñ LSTM':          [7.1, 7.5, 9.4, 14.1],
    'ÐžÐ´Ð½Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½Ñ‡Ð°Ñ‚Ð°Ñ LSTM Ñ Ñ€ÐµÐ³ÑƒÐ»ÑÑ€Ð¸Ð·Ð°Ñ†Ð¸ÐµÐ¹':   [7.3, 7.5, 8.7, 12.5],
    'Ð”Ð²ÑƒÑÑ‚ÑƒÐ¿ÐµÐ½Ñ‡Ð°Ñ‚Ð°Ñ Ð³Ð»ÑƒÐ±Ð¾ÐºÐ°Ñ LSTM':       [7.4, 7.6, 9.2, 13],
    'Ð›Ð°ÑÑÐ¾-Ñ€ÐµÐ³Ñ€ÐµÑÑÐ¸Ñ':   [8, 10.5, 15.1, 21.9],
    'Ð›Ð¸Ð½ÐµÐ¹Ð½Ð°Ñ Ñ€ÐµÐ³Ñ€ÐµÑÑÐ¸Ñ':  [8.7, 12.3, 16.1, 26.2],
    'Ð“Ñ€ÐµÐ±Ð½ÐµÐ²Ð°Ñ Ñ€ÐµÐ³Ñ€ÐµÑÑÐ¸Ñ':   [8.3, 11.2, 15.3, 23.4]
}

# Ð¦Ð²ÐµÑ‚Ð° Ð´Ð»Ñ ÐºÑ€Ð°ÑÐ¾Ñ‚Ñ‹
colors = {
    'Ð”Ð²ÑƒÐ½Ð°Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð½Ð°Ñ LSTM': '#d62728',
    'Ð£Ð¿Ñ€Ð¾Ñ‰ÐµÐ½Ð½Ð°Ñ Ð±Ð°Ð·Ð¾Ð²Ð°Ñ LSTM':          '#1f77b4',
    'ÐžÐ´Ð½Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½Ñ‡Ð°Ñ‚Ð°Ñ LSTM Ñ Ñ€ÐµÐ³ÑƒÐ»ÑÑ€Ð¸Ð·Ð°Ñ†Ð¸ÐµÐ¹':   '#ff7f0e',
    'Ð”Ð²ÑƒÑÑ‚ÑƒÐ¿ÐµÐ½Ñ‡Ð°Ñ‚Ð°Ñ Ð³Ð»ÑƒÐ±Ð¾ÐºÐ°Ñ LSTM':       '#2ca02c',
    'Ð›Ð°ÑÑÐ¾-Ñ€ÐµÐ³Ñ€ÐµÑÑÐ¸Ñ':      '#ff9896',
    #'Polynomial Regression':            '#98df8a',
    'Ð›Ð¸Ð½ÐµÐ¹Ð½Ð°Ñ Ñ€ÐµÐ³Ñ€ÐµÑÑÐ¸Ñ':  '#ffbb78',
    'Ð“Ñ€ÐµÐ±Ð½ÐµÐ²Ð°Ñ Ñ€ÐµÐ³Ñ€ÐµÑÑÐ¸Ñ':   '#98df8a'
}

# Ð¡Ñ‚Ñ€Ð¾Ð¸Ð¼ Ð³Ñ€Ð°Ñ„Ð¸Ðº
plt.figure(figsize=(12, 7))

for model_name, errors in mse_data.items():
    plt.plot(horizons, errors, marker='o', label=model_name, color=colors.get(model_name, None))

plt.title('Ð¡Ñ€Ð°Ð²Ð½ÐµÐ½Ð¸Ðµ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð¿Ð¾ MAPE Ð² Ð·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑ‚Ð¸ Ð¾Ñ‚ Ð³Ð¾Ñ€Ð¸Ð·Ð¾Ð½Ñ‚Ð° Ð¿Ñ€Ð¾Ð³Ð½Ð¾Ð·Ð°', fontsize=16)
plt.xlabel('Ð“Ð¾Ñ€Ð¸Ð·Ð¾Ð½Ñ‚ Ð¿Ñ€Ð¾Ð³Ð½Ð¾Ð·Ð° (Ð¿ÐµÑ€Ð¸Ð¾Ð´Ñ‹)', fontsize=14)
plt.ylabel('ÐžÑˆÐ¸Ð±ÐºÐ° (MAPE), %', fontsize=14)
plt.grid(True)
plt.legend()
plt.xticks(horizons)
plt.savefig('/content/drive/MyDrive/8 ÑÐµÐ¼ÐµÑÑ‚Ñ€/Ð”Ð¸Ð¿Ð»Ð¾Ð¼/Ð’Ñ‹Ð²Ð¾Ð´/mse_comparison_new.png',
                dpi=120, bbox_inches='tight')
plt.show()

import matplotlib.pyplot as plt

# Ð’Ñ…Ð¾Ð´Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ
mse_data = {
    'Ð”Ð²ÑƒÐ½Ð°Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð½Ð°Ñ LSTM': [0.002640654, 0.003889505, 0.00660325, 0.011556428],
    'Ð£Ð¿Ñ€Ð¾Ñ‰ÐµÐ½Ð½Ð°Ñ Ð±Ð°Ð·Ð¾Ð²Ð°Ñ LSTM': [0.002964542, 0.009032848, 0.008671352, 0.031456055],
    'ÐžÐ´Ð½Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½Ñ‡Ð°Ñ‚Ð°Ñ LSTM Ñ Ñ€ÐµÐ³ÑƒÐ»ÑÑ€Ð¸Ð·Ð°Ñ†Ð¸ÐµÐ¹': [0.003725677, 0.005479769, 0.007314557, 0.020705814],
    'Ð”Ð²ÑƒÑÑ‚ÑƒÐ¿ÐµÐ½Ñ‡Ð°Ñ‚Ð°Ñ Ð³Ð»ÑƒÐ±Ð¾ÐºÐ°Ñ LSTM': [0.004086467, 0.004830991, 0.008041487, 0.039676043],
    'Ð›Ð°ÑÑÐ¾-Ñ€ÐµÐ³Ñ€ÐµÑÑÐ¸Ñ': [0.00867619, 0.01093151, 0.00733351, 0.01234252],
    'Ð›Ð¸Ð½ÐµÐ¹Ð½Ð°Ñ Ñ€ÐµÐ³Ñ€ÐµÑÑÐ¸Ñ': [0.02175156, 0.02194493, 0.00744146, 0.01335177],
    'Ð“Ñ€ÐµÐ±Ð½ÐµÐ²Ð°Ñ Ñ€ÐµÐ³Ñ€ÐµÑÑÐ¸Ñ': [0.02174603, 0.02177307, 0.00763521, 0.01201772]
}

# Ð“Ð¾Ñ€Ð¸Ð·Ð¾Ð½Ñ‚Ñ‹ Ð¿Ñ€Ð¾Ð³Ð½Ð¾Ð·Ð° Ð¸ ÑÐ¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²ÑƒÑŽÑ‰ÐµÐµ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ñ‚Ð¾Ñ‡ÐµÐº
forecast_horizons = [1.5, 2.5, 5, 10]
forecast_steps = [3, 5, 10, 20]

# Ð Ð°ÑÑÑ‡Ð¸Ñ‚Ñ‹Ð²Ð°ÐµÐ¼ MSE Ð½Ð° ÑˆÐ°Ð³ Ð¿Ñ€Ð¾Ð³Ð½Ð¾Ð·Ð°
mse_per_step_data = {
    model: [mse / steps for mse, steps in zip(mse_list, forecast_steps)]
    for model, mse_list in mse_data.items()
}

colors = {
    'Ð”Ð²ÑƒÐ½Ð°Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð½Ð°Ñ LSTM': '#d62728',
    'Ð£Ð¿Ñ€Ð¾Ñ‰ÐµÐ½Ð½Ð°Ñ Ð±Ð°Ð·Ð¾Ð²Ð°Ñ LSTM':          '#1f77b4',
    'ÐžÐ´Ð½Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½Ñ‡Ð°Ñ‚Ð°Ñ LSTM Ñ Ñ€ÐµÐ³ÑƒÐ»ÑÑ€Ð¸Ð·Ð°Ñ†Ð¸ÐµÐ¹':   '#ff7f0e',
    'Ð”Ð²ÑƒÑÑ‚ÑƒÐ¿ÐµÐ½Ñ‡Ð°Ñ‚Ð°Ñ Ð³Ð»ÑƒÐ±Ð¾ÐºÐ°Ñ LSTM':       '#2ca02c',
    'Ð›Ð°ÑÑÐ¾-Ñ€ÐµÐ³Ñ€ÐµÑÑÐ¸Ñ':      '#ff9896',
    #'Polynomial Regression':            '#98df8a',
    'Ð›Ð¸Ð½ÐµÐ¹Ð½Ð°Ñ Ñ€ÐµÐ³Ñ€ÐµÑÑÐ¸Ñ':  '#ffbb78',
    'Ð“Ñ€ÐµÐ±Ð½ÐµÐ²Ð°Ñ Ñ€ÐµÐ³Ñ€ÐµÑÑÐ¸Ñ':   '#98df8a'
}

# Ð¡Ñ‚Ñ€Ð¾Ð¸Ð¼ Ð³Ñ€Ð°Ñ„Ð¸Ðº
plt.figure(figsize=(12, 7))
for model, mse_list in mse_per_step_data.items():
    plt.plot(forecast_horizons, mse_list, marker='o', label=model, color=colors.get(model, None))

plt.xlabel("Ð“Ð¾Ñ€Ð¸Ð·Ð¾Ð½Ñ‚ Ð¿Ñ€Ð¾Ð³Ð½Ð¾Ð·Ð° (Ð»ÐµÑ‚)")
plt.ylabel("Ð¡Ñ€ÐµÐ´Ð½ÑÑ Ð¾ÑˆÐ¸Ð±ÐºÐ° Ð½Ð° ÑˆÐ°Ð³ Ð¿Ñ€Ð¾Ð³Ð½Ð¾Ð·Ð° (MSE / Ñ‚Ð¾Ñ‡ÐºÐ°)")
plt.title("Ð¡Ñ€Ð°Ð²Ð½ÐµÐ½Ð¸Ðµ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð¿Ð¾ ÑÑ€ÐµÐ´Ð½ÐµÐ¹ Ð¾ÑˆÐ¸Ð±ÐºÐµ Ð½Ð° ÑˆÐ°Ð³ Ð¿Ñ€Ð¾Ð³Ð½Ð¾Ð·Ð°")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

def get_user_data():
    indicators = []
    for i in range(8):
        while True:
            try:
                values_input = input(f"Ð’Ð²ÐµÐ´Ð¸Ñ‚Ðµ 5 Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ð¹ Ð´Ð»Ñ Ð¿Ð¾ÐºÐ°Ð·Ð°Ñ‚ÐµÐ»Ñ {i + 1} Ñ‡ÐµÑ€ÐµÐ· Ð¿Ñ€Ð¾Ð±ÐµÐ»: ")
                values = list(map(float, values_input.split()))
                if len(values) != 5:
                    print("ÐžÑˆÐ¸Ð±ÐºÐ°: Ð½ÑƒÐ¶Ð½Ð¾ Ð²Ð²ÐµÑÑ‚Ð¸ Ñ€Ð¾Ð²Ð½Ð¾ 5 Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ð¹!")
                    continue
                indicators.append(values)
                break
            except ValueError:
                print("ÐžÑˆÐ¸Ð±ÐºÐ°: Ð²Ð²Ð¾Ð´Ð¸Ñ‚Ðµ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ñ‡Ð¸ÑÐ»Ð°, Ñ€Ð°Ð·Ð´ÐµÐ»ÐµÐ½Ð½Ñ‹Ðµ Ð¿Ñ€Ð¾Ð±ÐµÐ»Ð°Ð¼Ð¸!")
    return np.array(indicators)



def process_with_neural_network(data):
    # Ð—Ð´ÐµÑÑŒ Ð´Ð¾Ð»Ð¶Ð½Ð° Ð±Ñ‹Ñ‚ÑŒ Ð²Ð°ÑˆÐ° Ð½ÐµÐ¹Ñ€Ð¾ÑÐµÑ‚ÑŒ
    # Ð’ ÑÑ‚Ð¾Ð¼ Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ðµ Ð¿Ñ€Ð¾ÑÑ‚Ð¾ Ð²Ñ‹Ð²Ð¾Ð´Ð¸Ð¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ
    print("\nÐ”Ð°Ð½Ð½Ñ‹Ðµ, Ð¿ÐµÑ€ÐµÐ´Ð°Ð½Ð½Ñ‹Ðµ Ð² Ð½ÐµÐ¹Ñ€Ð¾ÑÐµÑ‚ÑŒ:")
    print(data)
    # ÐŸÑ€Ð¸Ð¼ÐµÑ€: return neural_network.predict(data)

if __name__ == "__main__":
    print("Ð’Ð²ÐµÐ´Ð¸Ñ‚Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ñ… Ð¿Ð¾ÐºÐ°Ð·Ð°Ñ‚ÐµÐ»ÐµÐ¹ Ð´Ð»Ñ Ð¿Ñ€Ð¾Ð³Ð½Ð¾Ð·Ð° Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ Ð¸Ð½Ð´ÐµÐºÑÐ° ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð° Ð¶Ð¸Ð·Ð½Ð¸:\n1. ÐŸÐ¾ÐºÑƒÐ¿Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð°Ñ ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð¾ÑÑ‚ÑŒ\n2. Ð‘ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾ÑÑ‚ÑŒ\n3. Ð—Ð´Ñ€Ð°Ð²Ð¾Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ\n4. Ð¡Ñ‚Ð¾Ð¸Ð¼Ð¾ÑÑ‚ÑŒ Ð¶Ð¸Ð·Ð¸\n5. Ð”Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð¾ÑÑ‚ÑŒ Ð¶Ð¸Ð»ÑŒÑ\n6. Ð’Ñ€ÐµÐ¼Ñ Ð² Ð¿ÑƒÑ‚Ð¸\n7. Ð—Ð°Ð³Ñ€ÑÐ·Ð½ÐµÐ½Ð¸Ðµ Ð¾ÐºÑ€ÑƒÐ¶Ð°ÑŽÑ‰ÐµÐ¹ ÑÑ€ÐµÐ´Ñ‹\n8. ÐšÐ»Ð¸Ð¼Ð°Ñ‚")
    user_data = get_user_data()
    process_with_neural_network(user_data)

import numpy as np
import pandas as pd
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from tensorflow.keras.models import load_model
from tensorflow.keras.metrics import MeanSquaredError

def get_user_data():
    """Ð—Ð°Ð¿Ñ€Ð°ÑˆÐ¸Ð²Ð°ÐµÑ‚ Ñƒ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ 5 Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ð¹ Ð´Ð»Ñ 8 Ð¿Ð¾ÐºÐ°Ð·Ð°Ñ‚ÐµÐ»ÐµÐ¹."""
    indicators = []
    print("Ð’Ð²ÐµÐ´Ð¸Ñ‚Ðµ 5 Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ð¹ Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ Ð¸Ð· 8 Ð¿Ð¾ÐºÐ°Ð·Ð°Ñ‚ÐµÐ»ÐµÐ¹ (Ñ€Ð°Ð·Ð´ÐµÐ»ÑÐ¹Ñ‚Ðµ Ð¿Ñ€Ð¾Ð±ÐµÐ»Ð¾Ð¼):")

    features = [
        "Purchasing Power Index",
        "Safety Index",
        "Health Care Index",
        "Cost of Living Index",
        "Property Price to Income Ratio",
        "Traffic Commute Time Index",
        "Pollution Index",
        "Climate Index"
    ]

    for i, feature in enumerate(features, 1):
        while True:
            try:
                values_input = input(f"{i}. {feature}: ")
                values = list(map(float, values_input.split()))
                if len(values) != 5:
                    print("ÐžÑˆÐ¸Ð±ÐºÐ°: Ð½ÑƒÐ¶Ð½Ð¾ Ð²Ð²ÐµÑÑ‚Ð¸ Ñ€Ð¾Ð²Ð½Ð¾ 5 Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ð¹!")
                    continue
                indicators.append(values)
                break
            except ValueError:
                print("ÐžÑˆÐ¸Ð±ÐºÐ°: Ð²Ð²Ð¾Ð´Ð¸Ñ‚Ðµ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ñ‡Ð¸ÑÐ»Ð°, Ñ€Ð°Ð·Ð´ÐµÐ»ÐµÐ½Ð½Ñ‹Ðµ Ð¿Ñ€Ð¾Ð±ÐµÐ»Ð°Ð¼Ð¸!")

    return np.array(indicators).T

def preprocess_user_data(data):
    """ÐŸÑ€ÐµÐ´Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð²Ð²ÐµÐ´ÐµÐ½Ð½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð°Ð½Ð°Ð»Ð¾Ð³Ð¸Ñ‡Ð½Ð¾ Ð²Ð°ÑˆÐµÐ¼Ñƒ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ñƒ."""
    df = pd.DataFrame(data, columns=[
        "Purchasing Power Index",
        "Safety Index",
        "Health Care Index",
        "Cost of Living Index",
        "Property Price to Income Ratio",
        "Traffic Commute Time Index",
        "Pollution Index",
        "Climate Index"
    ])

    df['Economic_Stress'] = df['Cost of Living Index'] / df['Purchasing Power Index']

    pca = PCA(n_components=1)
    df['Safety_Pollution_PC'] = pca.fit_transform(df[['Safety Index', 'Pollution Index']])

    df_final = df.drop(columns=[
        'Cost of Living Index', 'Purchasing Power Index',
        'Safety Index', 'Pollution Index',
        'Traffic Commute Time Index', 'Health Care Index'
    ])

    scaler = StandardScaler()
    scaled_data = scaler.fit_transform(df_final)

    return scaled_data.reshape(1, 5, -1)

if __name__ == "__main__":
    user_data = get_user_data()

    processed_data = preprocess_user_data(user_data)

    custom_objects = {
        'mse': MeanSquaredError(name='mse')
    }

    loaded_model = load_model(
        '/content/drive/MyDrive/8 ÑÐµÐ¼ÐµÑÑ‚Ñ€/Ð”Ð¸Ð¿Ð»Ð¾Ð¼/models/bidirectional_lstm.h5',
        custom_objects=custom_objects
    )

    y_pred_final = loaded_model.predict(processed_data)
    y_pred_original = scaler_y.inverse_transform(y_pred_final)
    print(y_pred_original)
    plt.plot(y_pred_original[0], marker='o')
    plt.title('ÐŸÑ€Ð¾Ð³Ð½Ð¾Ð· Ð½Ð° 10 Ð»ÐµÑ‚')
    plt.xlabel('Ð“Ð¾Ñ€Ð¸Ð·Ð¾Ð½Ñ‚ Ð¿Ñ€Ð¾Ð³Ð½Ð¾Ð·Ð° (ÑˆÐ°Ð³)')
    plt.ylabel('Ð—Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ')
    plt.grid(True)
    plt.tight_layout()
    plt.show()

print(y_pred_final[0])
print(y_pred_original[0])

plt.plot(y_pred_final[0], marker='o')
plt.title('ÐŸÑ€Ð¾Ð³Ð½Ð¾Ð· Ð½Ð° 10 Ð»ÐµÑ‚')
plt.xlabel('Ð“Ð¾Ñ€Ð¸Ð·Ð¾Ð½Ñ‚ Ð¿Ñ€Ð¾Ð³Ð½Ð¾Ð·Ð° (ÑˆÐ°Ð³)')
plt.ylabel('Ð—Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ')
plt.grid(True)
plt.tight_layout()
plt.show()

y_pred_final = loaded_model.predict(processed_data)
y_pred_original = scaler_y.inverse_transform(y_pred_final)
print(y_pred_original)
plt.plot(y_pred_original[0], marker='o')

plt.title('ÐŸÑ€Ð¾Ð³Ð½Ð¾Ð· Ð½Ð° 10 Ð»ÐµÑ‚')
plt.xlabel('Ð“Ð¾Ñ€Ð¸Ð·Ð¾Ð½Ñ‚ Ð¿Ñ€Ð¾Ð³Ð½Ð¾Ð·Ð° (ÑˆÐ°Ð³)')
plt.ylabel('Ð—Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ')
plt.grid(True)
plt.tight_layout()
plt.show()

# --- 1. ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ ---
MODELS = {
    "Base_LSTM": build_model_0,
    "Regularized_LSTM": build_model,
    "Stacked_LSTM": build_model_2,
    "Bidirectional_LSTM": build_model_4,
}

all_predictions = {}  # ÑÐ¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð²ÑÐµ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ñ

# --- 2. Ð¤ÑƒÐ½ÐºÑ†Ð¸Ñ Ð´Ð»Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð¸ ÑÐ±Ð¾Ñ€Ð° Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð² ---
def run_hyperparameter_tuning(models, scaled_X_train, y_train_scaled, scaled_X_test, y_test_scaled):
    results = {}
    all_histories = {}
    trained_models = {}  # ÐÐ¾Ð²Ñ‹Ð¹ ÑÐ»Ð¾Ð²Ð°Ñ€ÑŒ Ð´Ð»Ñ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹

    for model_name, hypermodel in models.items():
        print(f"\nðŸ”§ Tuning {model_name}")

        tuner = BayesianOptimization(
            hypermodel,
            objective='val_loss',
            max_trials=15,
            num_initial_points=3,
            directory='tuning',
            project_name=model_name
        )

        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

        tuner.search(
            scaled_X_train,
            y_train_scaled,
            epochs=100,
            validation_split=0.2,
            callbacks=[early_stopping],
            verbose=1
        )

        best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]
        best_model = tuner.hypermodel.build(best_hps)

        history = best_model.fit(
            scaled_X_train,
            y_train_scaled,
            epochs=100,
            validation_split=0.2,
            callbacks=[early_stopping],
            verbose=1
        )

        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð² ÑÐ»Ð¾Ð²Ð°Ñ€ÑŒ
        trained_models[model_name] = best_model

        # ÐžÑÑ‚Ð°Ð»ÑŒÐ½Ð¾Ð¹ ÐºÐ¾Ð´ Ð¾ÑÑ‚Ð°ÐµÑ‚ÑÑ Ð±ÐµÐ· Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ð¹...
        eval_result = best_model.evaluate(scaled_X_test, y_test_scaled)
        y_pred = best_model.predict(scaled_X_test)
        all_predictions[model_name] = y_pred

        results[model_name] = {
            'best': {
                'mse': mean_squared_error(y_test_scaled, y_pred),
                'mae': mean_absolute_error(y_test_scaled, y_pred),
                'params': best_hps.values,
                'epochs': len(history.history['val_loss']),
            }
        }
        all_histories[model_name] = history.history['val_loss']

    return results, trained_models  # Ð¢ÐµÐ¿ÐµÑ€ÑŒ Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÐ¼ Ð¸ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹, Ð¸ Ð¼Ð¾Ð´ÐµÐ»Ð¸

# --- 3. Ð¤ÑƒÐ½ÐºÑ†Ð¸Ñ Ð´Ð»Ñ Ð²Ð¸Ð·ÑƒÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ð¾Ð´Ð½Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸ ---
def plot_model_history(model_name, history):
    plt.figure(figsize=(10, 5))
    epochs = range(1, len(history.history['val_loss']) + 1)

    plt.plot(epochs, history.history['val_loss'], label='Validation Loss')
    plt.plot(epochs, history.history['loss'], label='Training Loss')
    plt.title(f'Ð˜Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¸ Ð¿Ð¾Ñ‚ÐµÑ€ÑŒ Ð¿Ð¾ ÑÐ¿Ð¾Ñ…Ð°Ð¼ Ð´Ð»Ñ {model_name}', pad=20)
    plt.xlabel('Ð­Ð¿Ð¾Ñ…Ð°', labelpad=10)
    plt.ylabel('Ð—Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¸ Ð¿Ð¾Ñ‚ÐµÑ€ÑŒ (MSE)', labelpad=10)
    plt.legend()
    plt.grid(True, linestyle='--', alpha=0.6)

    plt.tight_layout()
    plt.savefig(f'/content/drive/MyDrive/8 ÑÐµÐ¼ÐµÑÑ‚Ñ€/Ð”Ð¸Ð¿Ð»Ð¾Ð¼/Ð’Ñ‹Ð²Ð¾Ð´/Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ðµ Ð½Ð° 20/ÐÐ¿Ð³Ñ€ÐµÐ¹Ð´/{model_name}_history.png', dpi=120)
    plt.close()

# --- 4. Ð¤ÑƒÐ½ÐºÑ†Ð¸Ñ Ð´Ð»Ñ Ð¾Ð±Ñ‰ÐµÐ³Ð¾ ÑÑ€Ð°Ð²Ð½ÐµÐ½Ð¸Ñ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ ---
def plot_combined_val_loss(histories):
    plt.figure(figsize=(12, 8))

    max_epochs = max(len(h) for h in histories.values())

    for model_name, val_loss in histories.items():
        epochs = range(1, len(val_loss) + 1)
        plt.plot(epochs, val_loss, label=model_name, linewidth=2)

    plt.title('Ð¡Ñ€Ð°Ð²Ð½ÐµÐ½Ð¸Ðµ Ð´Ð¸Ð½Ð°Ð¼Ð¸ÐºÐ¸ Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ð¾Ð½Ð½Ð¾Ð¹ Ð¾ÑˆÐ¸Ð±ÐºÐ¸ (Loss) Ð´Ð»Ñ Ñ€Ð°Ð·Ð½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹', pad=20)
    plt.xlabel('Ð­Ð¿Ð¾Ñ…Ð°', labelpad=10)
    plt.ylabel('Ð—Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ð¾Ð½Ð½Ð¾Ð¹ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¸ Ð¿Ð¾Ñ‚ÐµÑ€ÑŒ (MSE)', labelpad=10)
    plt.legend()
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.xlim(1, max_epochs)

    plt.axhline(y=min(min(h) for h in histories.values()),
                color='gray', linestyle=':', alpha=0.5)

    plt.tight_layout()
    plt.savefig('/content/drive/MyDrive/8 ÑÐµÐ¼ÐµÑÑ‚Ñ€/Ð”Ð¸Ð¿Ð»Ð¾Ð¼/Ð’Ñ‹Ð²Ð¾Ð´/Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ðµ Ð½Ð° 20/ÐÐ¿Ð³Ñ€ÐµÐ¹Ð´/models_val_loss_comparison.png',
                dpi=120, bbox_inches='tight')
    plt.close()

# --- 5. Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð² Excel ---
def save_results_to_excel(results):
    rows = []

    for model, data in results.items():
        rows.append({
            'Model': model,
            'MSE': data['best']['mse'],
            'MAE': data['best']['mae'],
            'Epochs': data['best']['epochs'],
            'Params': json.dumps(data['best']['params'])
        })

    df = pd.DataFrame(rows)
    df = df.sort_values(by='MSE')

    output_path = '/content/drive/MyDrive/8 ÑÐµÐ¼ÐµÑÑ‚Ñ€/Ð”Ð¸Ð¿Ð»Ð¾Ð¼/Ð’Ñ‹Ð²Ð¾Ð´/Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ðµ Ð½Ð° 20/ÐÐ¿Ð³Ñ€ÐµÐ¹Ð´/final_results.xlsx'
    df.to_excel(output_path, index=False)

    print(f"\nâœ… Excel Ñ„Ð°Ð¹Ð» ÑÐ¾Ñ…Ñ€Ð°Ð½Ñ‘Ð½ Ð¿Ð¾ Ð°Ð´Ñ€ÐµÑÑƒ: {output_path}")
    print("\nðŸ“Š TOP-3 Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð¿Ð¾ MSE:")
    print(df.head(3))

# Ð—Ð°Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¸ Ð¿Ð¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð¼Ð¾Ð´ÐµÐ»Ð¸
final_results, trained_models = run_hyperparameter_tuning(
    MODELS,
    scaled_X_train_20,
    y_train_scaled_20,
    scaled_X_test_20,
    y_test_scaled_20
)

# Ð”Ð¾ÑÑ‚Ð°ÐµÐ¼ Bidirectional LSTM
bidirectional_lstm_model = trained_models["Bidirectional_LSTM"]

trained_models

bidirectional_lstm_model.save('/content/drive/MyDrive/8 ÑÐµÐ¼ÐµÑÑ‚Ñ€/Ð”Ð¸Ð¿Ð»Ð¾Ð¼/models/bidirectional_lstm.h5')

from tensorflow.keras.models import load_model
from tensorflow.keras.metrics import MeanSquaredError

# Ð£ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼ ÐºÐ°ÑÑ‚Ð¾Ð¼Ð½Ñ‹Ðµ Ð¾Ð±ÑŠÐµÐºÑ‚Ñ‹ (ÐµÑÐ»Ð¸ Ð¾Ð½Ð¸ ÐµÑÑ‚ÑŒ)
custom_objects = {
    'mse': MeanSquaredError(name='mse'),  # Ð¯Ð²Ð½Ð¾ Ñ€ÐµÐ³Ð¸ÑÑ‚Ñ€Ð¸Ñ€ÑƒÐµÐ¼ MSE
    # Ð”Ð¾Ð±Ð°Ð²ÑŒÑ‚Ðµ Ð´Ñ€ÑƒÐ³Ð¸Ðµ ÐºÐ°ÑÑ‚Ð¾Ð¼Ð½Ñ‹Ðµ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸/ÑÐ»Ð¾Ð¸ Ð·Ð´ÐµÑÑŒ
}

loaded_model = load_model(
    '/content/drive/MyDrive/8 ÑÐµÐ¼ÐµÑÑ‚Ñ€/Ð”Ð¸Ð¿Ð»Ð¾Ð¼/models/bidirectional_lstm.h5',
    custom_objects=custom_objects
)

sample_input = scaled_X_test_20[0:1]  # Ð‘ÐµÑ€ÐµÐ¼ Ð¿ÐµÑ€Ð²Ñ‹Ð¹ Ð¿Ñ€Ð¸Ð¼ÐµÑ€ Ð¸Ð· Ñ‚ÐµÑÑ‚Ð¾Ð²Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…
prediction = loaded_model.predict(sample_input)
print(y_pred_final)

data[data['Country']=='Luxembourg']

import numpy as np
import pandas as pd
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from tensorflow.keras.models import load_model
from tensorflow.keras.metrics import MeanSquaredError

def get_user_data():
    """Ð—Ð°Ð¿Ñ€Ð°ÑˆÐ¸Ð²Ð°ÐµÑ‚ Ñƒ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ 5 Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ð¹ Ð´Ð»Ñ 8 Ð¿Ð¾ÐºÐ°Ð·Ð°Ñ‚ÐµÐ»ÐµÐ¹."""
    indicators = []
    print("Ð’Ð²ÐµÐ´Ð¸Ñ‚Ðµ 5 Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ð¹ Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ Ð¸Ð· 8 Ð¿Ð¾ÐºÐ°Ð·Ð°Ñ‚ÐµÐ»ÐµÐ¹ (Ñ€Ð°Ð·Ð´ÐµÐ»ÑÐ¹Ñ‚Ðµ Ð¿Ñ€Ð¾Ð±ÐµÐ»Ð¾Ð¼):")

    features = [
        "Purchasing Power Index",
        "Safety Index",
        "Health Care Index",
        "Cost of Living Index",
        "Property Price to Income Ratio",
        "Traffic Commute Time Index",
        "Pollution Index",
        "Climate Index"
    ]

    for i, feature in enumerate(features, 1):
        while True:
            try:
                values_input = input(f"{i}. {feature}: ")
                values = list(map(float, values_input.split()))
                if len(values) != 5:
                    print("ÐžÑˆÐ¸Ð±ÐºÐ°: Ð½ÑƒÐ¶Ð½Ð¾ Ð²Ð²ÐµÑÑ‚Ð¸ Ñ€Ð¾Ð²Ð½Ð¾ 5 Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ð¹!")
                    continue
                indicators.append(values)
                break
            except ValueError:
                print("ÐžÑˆÐ¸Ð±ÐºÐ°: Ð²Ð²Ð¾Ð´Ð¸Ñ‚Ðµ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ñ‡Ð¸ÑÐ»Ð°, Ñ€Ð°Ð·Ð´ÐµÐ»ÐµÐ½Ð½Ñ‹Ðµ Ð¿Ñ€Ð¾Ð±ÐµÐ»Ð°Ð¼Ð¸!")

    return np.array(indicators).T

def preprocess_user_data(data):
    """ÐŸÑ€ÐµÐ´Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð²Ð²ÐµÐ´ÐµÐ½Ð½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð°Ð½Ð°Ð»Ð¾Ð³Ð¸Ñ‡Ð½Ð¾ Ð²Ð°ÑˆÐµÐ¼Ñƒ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ñƒ."""
    df = pd.DataFrame(data, columns=[
        "Purchasing Power Index",
        "Safety Index",
        "Health Care Index",
        "Cost of Living Index",
        "Property Price to Income Ratio",
        "Traffic Commute Time Index",
        "Pollution Index",
        "Climate Index"
    ])

    df['Economic_Stress'] = df['Cost of Living Index'] / df['Purchasing Power Index']

    #pca = PCA(n_components=1)
    df['Safety_Pollution_PC'] = pca.transform(df[['Safety Index', 'Pollution Index']])

    df_final = df.drop(columns=[
        'Cost of Living Index', 'Purchasing Power Index',
        'Safety Index', 'Pollution Index',
        'Traffic Commute Time Index', 'Health Care Index'
    ])

    #scaler = StandardScaler()
    scaled_data = scaler_x.transform(df_final)

    return scaled_data.reshape(1, 5, -1)

if __name__ == "__main__":
    user_data = get_user_data()

    processed_data = preprocess_user_data(user_data)

    custom_objects = {
        'mse': MeanSquaredError(name='mse')
    }

    loaded_model = load_model(
        '/content/drive/MyDrive/8 ÑÐµÐ¼ÐµÑÑ‚Ñ€/Ð”Ð¸Ð¿Ð»Ð¾Ð¼/models/bidirectional_lstm.h5',
        custom_objects=custom_objects
    )

    y_pred_final = loaded_model.predict(processed_data)
    y_pred_original = scaler_y.inverse_transform(y_pred_final)
    print(y_pred_original)
    plt.plot(y_pred_original[0], marker='o')
    plt.title('ÐŸÑ€Ð¾Ð³Ð½Ð¾Ð· Ð½Ð° 10 Ð»ÐµÑ‚')
    plt.xlabel('Ð“Ð¾Ñ€Ð¸Ð·Ð¾Ð½Ñ‚ Ð¿Ñ€Ð¾Ð³Ð½Ð¾Ð·Ð° (ÑˆÐ°Ð³)')
    plt.ylabel('Ð—Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ')
    plt.grid(True)
    plt.tight_layout()
    plt.show()

import numpy as np
import pandas as pd
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from tensorflow.keras.models import load_model
from tensorflow.keras.metrics import MeanSquaredError

custom_objects = {
    'mse': MeanSquaredError(name='mse')
}

loaded_model = load_model(
   '/content/drive/MyDrive/8 ÑÐµÐ¼ÐµÑÑ‚Ñ€/Ð”Ð¸Ð¿Ð»Ð¾Ð¼/models/bidirectional_lstm.h5',
    custom_objects=custom_objects
)

# ÐÐ°Ð¹Ð´Ñ‘Ð¼ ÑÐ»Ð¾Ð¹ Bidirectional
bidir_layer = loaded_model.layers[0]

# ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ð¼ forward-ÑÐ»Ð¾Ð¹ Ð¸Ð· Ð´Ð²ÑƒÐ½Ð°Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð½Ð¾Ð³Ð¾ ÑÐ»Ð¾Ñ
forward_lstm = bidir_layer.forward_layer

# Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ Ð²ÐµÑÐ° Ð¸Ð· forward-Ñ‡Ð°ÑÑ‚Ð¸
forward_weights = forward_lstm.get_weights()

# Ð£ÐºÐ°Ð¶Ð¸ Ð½ÑƒÐ¶Ð½Ñ‹Ðµ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ Ð²Ñ€ÑƒÑ‡Ð½ÑƒÑŽ Ð¸Ð»Ð¸ Ð¿Ð¾ Ð°Ð½Ð°Ð»Ð¾Ð³Ð¸Ð¸ Ñ hp
units = forward_lstm.units
#input_shape = forward_lstm.input_shape[1:]  # (timesteps, features)

# Ð¡Ð¾Ð·Ð´Ð°Ñ‘Ð¼ Ð¾Ð±Ñ‹Ñ‡Ð½ÑƒÑŽ forward-Ð¼Ð¾Ð´ÐµÐ»ÑŒ
forward_model = Sequential()
forward_model.add(layers.LSTM(units,
                       recurrent_dropout=forward_lstm.recurrent_dropout,
                       input_shape=(scaled_X_train.shape[1], scaled_X_train.shape[2])))
forward_model.add(Dropout(0.5))

# Ð”Ð¾Ð±Ð°Ð²ÑŒ Dense-ÑÐ»Ð¾Ð¸ ÐºÐ°Ðº Ð² Ð¸ÑÑ…Ð¾Ð´Ð½Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸
forward_model.add(Dense(16, kernel_regularizer=keras.regularizers.l1_l2(l1=1e-5, l2=0.000797)))
forward_model.add(Dense(20, activation='linear'))  # Ð²Ñ‹Ñ…Ð¾Ð´Ð½Ð¾Ð¹ ÑÐ»Ð¾Ð¹

# ÐšÐ¾Ð¼Ð¿Ð¸Ð»ÑÑ†Ð¸Ñ
forward_model.compile(optimizer='adam', loss='mse', metrics=['mae'])

# Ð£ÑÑ‚Ð°Ð½Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÐ¼ Ð²ÐµÑÐ° Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð² Ð¿ÐµÑ€Ð²Ð¾Ð¼ LSTM-ÑÐ»Ð¾Ðµ
forward_model.layers[0].set_weights(forward_weights)
forward_model.summary()

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, Dense, Dropout
from tensorflow.keras import initializers

# Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð½ÑƒÑŽ BiLSTM-Ð¼Ð¾Ð´ÐµÐ»ÑŒ
loaded_model = keras.models.load_model(
    '/content/drive/MyDrive/8 ÑÐµÐ¼ÐµÑÑ‚Ñ€/Ð”Ð¸Ð¿Ð»Ð¾Ð¼/models/bidirectional_lstm.h5',
    custom_objects=custom_objects
)

# Forward LSTM
forward_lstm = loaded_model.layers[0].forward_layer

# Ð’Ñ…Ð¾Ð´
input_layer = Input(shape=(scaled_X_train.shape[1], scaled_X_train.shape[2]))
forward_output = forward_lstm(input_layer)

# Dropout â€” Ð¼Ð¾Ð¶ÐµÐ¼ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ ÐºÐ°Ðº ÐµÑÑ‚ÑŒ
x = Dropout(0.5)(forward_output)

# Ð¡Ð¾Ð·Ð´Ð°Ñ‘Ð¼ Dense Ð·Ð°Ð½Ð¾Ð²Ð¾: Ð²Ñ…Ð¾Ð´ Ñ‚ÐµÐ¿ÐµÑ€ÑŒ (None, 32) Ð²Ð¼ÐµÑÑ‚Ð¾ (None, 64)
# ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ð¼ ÑÑ‚Ð°Ñ€Ñ‹Ð¹ Dense-ÑÐ»Ð¾Ð¹
old_dense = loaded_model.layers[2]

# ÐÐ¾Ð²Ñ‹Ð¹ Dense Ñ Ñ‚ÐµÐ¼Ð¸ Ð¶Ðµ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð°Ð¼Ð¸, Ð½Ð¾ input_dim=32
new_dense = Dense(
    old_dense.units,
    activation=old_dense.activation,
    kernel_regularizer=old_dense.kernel_regularizer,
    bias_regularizer=old_dense.bias_regularizer,
    kernel_initializer=initializers.Zeros(),  # Ð¿Ð¾Ñ‚Ð¾Ð¼ Ð¿ÐµÑ€ÐµÐ·Ð°Ð¿Ð¸ÑˆÐµÐ¼ Ð²ÐµÑÐ°Ð¼Ð¸
    bias_initializer=initializers.Zeros()
)

x = new_dense(x)

# Ð’Ñ‹Ñ…Ð¾Ð´Ð½Ð¾Ð¹ Dense (ÐµÐ³Ð¾ Ð¼Ð¾Ð¶Ð½Ð¾ Ð²Ð·ÑÑ‚ÑŒ ÐºÐ°Ðº ÐµÑÑ‚ÑŒ â€” Ð¾Ð½ Ð¿Ð¾Ð´ÐºÐ»ÑŽÑ‡Ð°ÐµÑ‚ÑÑ Ðº (None, 16))
output_dense = loaded_model.layers[3]
output = output_dense(x)

# Ð¡Ð¾Ð±Ð¸Ñ€Ð°ÐµÐ¼ forward-Ð¼Ð¾Ð´ÐµÐ»ÑŒ
forward_model = Model(inputs=input_layer, outputs=output)

# Ð£ÑÑ‚Ð°Ð½Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÐ¼ Ð²ÐµÑÐ°: Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð¿ÐµÑ€Ð²ÑƒÑŽ Ð¿Ð¾Ð»Ð¾Ð²Ð¸Ð½Ñƒ Ð²ÐµÑÐ¾Ð² (Ñ‚.Ðµ. forward)
dense_weights = old_dense.get_weights()
# Ð²ÐµÑÐ°: [W, b] Ð³Ð´Ðµ W.shape = (64, 16), b.shape = (16,)
new_dense.set_weights([
    dense_weights[0][:32, :],  # Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð¿ÐµÑ€Ð²Ð°Ñ Ð¿Ð¾Ð»Ð¾Ð²Ð¸Ð½Ð° Ð²ÐµÑÐ¾Ð² (forward)
    dense_weights[1]           # bias Ð¼Ð¾Ð¶Ð½Ð¾ Ð¾ÑÑ‚Ð°Ð²Ð¸Ñ‚ÑŒ Ð±ÐµÐ· Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ð¹
])

forward_model.summary()

import numpy as np
import pandas as pd
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from tensorflow.keras.models import load_model
from tensorflow.keras.metrics import MeanSquaredError

def get_user_data():
    """Ð—Ð°Ð¿Ñ€Ð°ÑˆÐ¸Ð²Ð°ÐµÑ‚ Ñƒ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ 5 Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ð¹ Ð´Ð»Ñ 8 Ð¿Ð¾ÐºÐ°Ð·Ð°Ñ‚ÐµÐ»ÐµÐ¹."""
    indicators = []
    print("Ð’Ð²ÐµÐ´Ð¸Ñ‚Ðµ 5 Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ð¹ Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ Ð¸Ð· 8 Ð¿Ð¾ÐºÐ°Ð·Ð°Ñ‚ÐµÐ»ÐµÐ¹ (Ñ€Ð°Ð·Ð´ÐµÐ»ÑÐ¹Ñ‚Ðµ Ð¿Ñ€Ð¾Ð±ÐµÐ»Ð¾Ð¼):")

    features = [
        "Purchasing Power Index",
        "Safety Index",
        "Health Care Index",
        "Cost of Living Index",
        "Property Price to Income Ratio",
        "Traffic Commute Time Index",
        "Pollution Index",
        "Climate Index"
    ]

    for i, feature in enumerate(features, 1):
        while True:
            try:
                values_input = input(f"{i}. {feature}: ")
                values = list(map(float, values_input.split()))
                if len(values) != 5:
                    print("ÐžÑˆÐ¸Ð±ÐºÐ°: Ð½ÑƒÐ¶Ð½Ð¾ Ð²Ð²ÐµÑÑ‚Ð¸ Ñ€Ð¾Ð²Ð½Ð¾ 5 Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ð¹!")
                    continue
                indicators.append(values)
                break
            except ValueError:
                print("ÐžÑˆÐ¸Ð±ÐºÐ°: Ð²Ð²Ð¾Ð´Ð¸Ñ‚Ðµ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ñ‡Ð¸ÑÐ»Ð°, Ñ€Ð°Ð·Ð´ÐµÐ»ÐµÐ½Ð½Ñ‹Ðµ Ð¿Ñ€Ð¾Ð±ÐµÐ»Ð°Ð¼Ð¸!")

    return np.array(indicators).T

def preprocess_user_data(data):
    """ÐŸÑ€ÐµÐ´Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð²Ð²ÐµÐ´ÐµÐ½Ð½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð°Ð½Ð°Ð»Ð¾Ð³Ð¸Ñ‡Ð½Ð¾ Ð²Ð°ÑˆÐµÐ¼Ñƒ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ñƒ."""
    df = pd.DataFrame(data, columns=[
        "Purchasing Power Index",
        "Safety Index",
        "Health Care Index",
        "Cost of Living Index",
        "Property Price to Income Ratio",
        "Traffic Commute Time Index",
        "Pollution Index",
        "Climate Index"
    ])

    df['Economic_Stress'] = df['Cost of Living Index'] / df['Purchasing Power Index']

    #pca = PCA(n_components=1)
    df['Safety_Pollution_PC'] = pca.transform(df[['Safety Index', 'Pollution Index']])

    df_final = df.drop(columns=[
        'Cost of Living Index', 'Purchasing Power Index',
        'Safety Index', 'Pollution Index',
        'Traffic Commute Time Index', 'Health Care Index'
    ])

    #scaler = StandardScaler()
    scaled_data = scaler_x.transform(df_final)

    return scaled_data.reshape(1, 5, -1)

if __name__ == "__main__":
    user_data = get_user_data()

    processed_data = preprocess_user_data(user_data)

    custom_objects = {
        'mse': MeanSquaredError(name='mse')
    }

    loaded_model = forward_model

    y_pred_final = loaded_model.predict(processed_data)
    y_pred_original = scaler_y.inverse_transform(y_pred_final)
    print(y_pred_original)
    plt.plot(y_pred_original[0], marker='o')
    plt.title('ÐŸÑ€Ð¾Ð³Ð½Ð¾Ð· Ð½Ð° 10 Ð»ÐµÑ‚')
    plt.xlabel('Ð“Ð¾Ñ€Ð¸Ð·Ð¾Ð½Ñ‚ Ð¿Ñ€Ð¾Ð³Ð½Ð¾Ð·Ð° (ÑˆÐ°Ð³)')
    plt.ylabel('Ð—Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ')
    plt.grid(True)
    plt.tight_layout()
    plt.show()

def build_model_4(hp):
    model = keras.Sequential()

    # Ð”Ð²ÑƒÐ½Ð°Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð½Ð°Ñ LSTM
    model.add(layers.Bidirectional(
        layers.LSTM(
            hp.Int('units', 32, 128, step=32),
            recurrent_dropout=hp.Float('rec_drop', 0.1, 0.3)),
        input_shape=(scaled_X_train.shape[1], scaled_X_train.shape[2])))

    # Ð ÐµÐ³ÑƒÐ»ÑÑ€Ð¸Ð·Ð°Ñ†Ð¸Ñ
    model.add(layers.Dropout(hp.Float('dropout', 0.2, 0.5)))

    # Dense-ÑÐ»Ð¾Ð¸ Ñ L1/L2 Ñ€ÐµÐ³ÑƒÐ»ÑÑ€Ð¸Ð·Ð°Ñ†Ð¸ÐµÐ¹
    for i in range(hp.Int('num_dense', 1, 2)):
        model.add(layers.Dense(
            hp.Int(f'dense_{i}_units', 16, 64, step=16),
            kernel_regularizer=keras.regularizers.l1_l2(
                l1=hp.Float(f'l1_{i}', 1e-5, 1e-3),
                l2=hp.Float(f'l2_{i}', 1e-5, 1e-3))))

    # Ð’Ñ‹Ñ…Ð¾Ð´Ð½Ð¾Ð¹ ÑÐ»Ð¾Ð¹
    model.add(layers.Dense(20, activation='linear'))

    model.compile(
        optimizer=keras.optimizers.AdamW(
            hp.Float('lr', 1e-5, 1e-3),
            weight_decay=hp.Float('wd', 1e-6, 1e-4)),
        loss='mse',
        metrics=['mae'])
    return model